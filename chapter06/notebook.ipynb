{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Advanced RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
     "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
     "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
     "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# rag_ai_agent_book/.envから環境変数を読み込む\n",
    "dotenv_path = os.path.join(os.getcwd(), 'rag_ai_agent_book', '.env')\n",
    "\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-ai-agent-book\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. ハンズオンの準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "# langchain_communityパッケージからGitLoaderをインポート\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "# .mdxファイルのみを対象とするフィルタ関数を定義\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "# GitリポジトリからドキュメントをロードするためのGitLoaderインスタンスを作成\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",  # クローン元のリポジトリURL\n",
    "    repo_path=\"./langchain\",                                # ローカルにクローンするパス\n",
    "    branch=\"master\",                                        # 対象ブランチ\n",
    "    file_filter=file_filter,                                # ファイルフィルタ関数\n",
    ")\n",
    "\n",
    "# GitLoaderでドキュメントをロード\n",
    "documents = loader.load()\n",
    "# ロードしたドキュメント数を出力\n",
    "print(len(documents))\n",
    "\n",
    "# langchain_chromaからChroma、langchain_openaiからOpenAIEmbeddingsをインポート\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAIの埋め込みモデル（text-embedding-3-small）を初期化\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# ドキュメントと埋め込みモデルからChromaベクトルDBを作成\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下の3つの主要なステージがあります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築します。LangGraphを利用することで、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **プロダクション化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできるようにします。\\n\\n3. **デプロイメント**: LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換します。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクターストアなど）に対する標準インターフェースを実装し、数百のプロバイダーと統合しています。また、複数のオープンソースライブラリで構成されており、ユーザーは簡単にアプリケーションを構築、デプロイ、管理することができます。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文字列出力用のパーサーをインポート\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# チャットプロンプトテンプレートをインポート\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# 入力をそのまま渡すためのランナブルパススルーをインポート\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# OpenAIのチャットモデルをインポート\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# プロンプトテンプレートを作成（from_templateメソッドを使用）\n",
    "prompt=ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "文脈:\"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "質問:{question}\n",
    "''')\n",
    "\n",
    "# OpenAIのgpt-4o-miniモデルを初期化（temperature=0で決定論的出力）\n",
    "model=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "# ベクトルDBからレトリバーを作成\n",
    "retriever=db.as_retriever()\n",
    "# チェーンを構築（questionはそのまま渡し、contextはretriverで取得）\n",
    "chain={\n",
    "\t\"question\":RunnablePassthrough(), # 質問はそのまま渡す\n",
    "\t\"context\":retriever,              # contextにはretriverを指定\n",
    "}|prompt|model|StrOutputParser()     # プロンプト→モデル→出力パーサーの順にパイプ\n",
    "\n",
    "# チェーンに質問を投げて実行\n",
    "chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. 検索クエリの工夫\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE（Hypothetical Document Embeddings）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、開発者が推論を行うアプリケーションを簡単に構築できるようにすることを目的としたPythonパッケージおよび企業です。元々は単一のオープンソースパッケージとして始まりましたが、現在は企業とエコシステム全体に進化しています。\\n\\nLangChainの主な特徴には以下の3つがあります：\\n\\n1. **標準化されたコンポーネントインターフェース**: AIアプリケーションに必要なさまざまなモデルや関連コンポーネントのAPIが多様化しているため、開発者がプロバイダー間で切り替えたり、コンポーネントを組み合わせたりするのが難しくなっています。LangChainは、主要なコンポーネントのための標準インターフェースを提供し、プロバイダー間の切り替えを容易にします。\\n\\n2. **オーケストレーション**: アプリケーションが複雑になるにつれて、複数のコンポーネントやモデルを効率的に接続する必要があります。LangChainは、これらの要素を制御フローに組み込むためのオーケストレーション機能を提供します。\\n\\n3. **可観測性と評価**: アプリケーションが複雑になると、その内部で何が起こっているのかを理解するのが難しくなります。LangChainは、開発者がアプリケーションを監視し、迅速に評価を行うためのツールを提供します。\\n\\nLangChainは、個々のコンポーネントを単独で使用することも可能であり、開発者は自分のユースケースに最適なコンポーネントを選択できます。また、LangGraphというライブラリを使用することで、複雑なアプリケーションのオーケストレーションが可能になります。さらに、LangSmithというプラットフォームを通じて、アプリケーションの可観測性と評価をサポートしています。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyDe(HypotheticalDocumentEmbeddings)を利用したChatシステム\n",
    "# 質問に対する類似ドキュメントではなく、仮回答に対応するドキュメントを利用する\n",
    "\n",
    "# プロンプトのテンプレートを定義\n",
    "hypothetical_prompt=ChatPromptTemplate.from_template(\"\"\"\\\n",
    "次の質問に回答する一文を書いてください。\n",
    "質問:{question}\n",
    " \"\"\")\n",
    "# プロンプト、モデル、出力パーサーをパイプでつなぎ、仮回答生成チェーンを作成\n",
    "hypothetical_chain=hypothetical_prompt|model|StrOutputParser()\n",
    "\n",
    "# HyDE方式のRAGチェーンを作成（質問はそのまま、contextは仮回答→retriverで取得）\n",
    "hyde_rag_chain={\n",
    "\t\"question\":RunnablePassthrough(), # 質問はそのまま渡す\n",
    "\t\"context\":hypothetical_chain|retriever # contextには仮回答をretriverに渡して取得\n",
    "}|prompt|model|StrOutputParser() # プロンプト→モデル→出力パーサーの順にパイプ\n",
    "\n",
    "# チェーンに質問を投げて実行\n",
    "hyde_rag_chain.invoke(\"LangChainの概要を教えて\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数の検索クエリの生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、開発、運用、デプロイの各段階を簡素化することを目的としています。以下はLangChainの主な特徴です。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **運用化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを本番環境向けのAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなプロバイダーと統合できる標準インターフェースを実装しており、開発者が異なるコンポーネントを簡単に切り替えたり、組み合わせたりできるようにしています。また、複雑なアプリケーションのオーケストレーションを行うためのLangGraphや、アプリケーションの可視化と評価を行うLangSmithといったツールも提供しています。\\n\\n全体として、LangChainはAIアプリケーションの開発を効率化し、開発者がより迅速に高品質なアプリケーションを構築できるようにすることを目指しています。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pydanticのBaseModelとFieldをインポート\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 検索クエリのリストを格納するための出力モデルを定義\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "\tqueries:list[str]=Field(...,description=\"検索クエリのリスト\") # 検索クエリのリストを格納\n",
    "\n",
    "# 検索クエリ生成用のプロンプトテンプレートを作成\n",
    "query_generation_prompt=ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に対してベクターデータベースから関連文章を検索するために、\n",
    "3つの異なる検索クエリを生成せよ。\n",
    "距離ベースの類似性検索の限界を克服するために、\n",
    "ユーザーの質問に対して複数の視点を提供することが目標である。\n",
    "質問:{question}\n",
    "\"\"\") # プロンプトテンプレートの内容を指定\n",
    "\n",
    "# プロンプト→モデル→出力整形のチェーンを構築\n",
    "query_generation_chain=(\n",
    "\tquery_generation_prompt\n",
    "\t|model.with_structured_output(QueryGenerationOutput) # モデルの出力を構造化\n",
    "\t|(lambda x:x.queries) # queriesリストのみを抽出\n",
    ")\n",
    "\n",
    "# 複数クエリによるRAGチェーンを定義\n",
    "multi_query_rag_chain={\n",
    "\t\"question\":RunnablePassthrough(), # 質問をそのまま渡す\n",
    "\t\"context\":query_generation_chain|retriever.map() # 生成したクエリで検索を実行\n",
    "}|prompt|model|StrOutputParser() # プロンプト→モデル→出力パース\n",
    "\n",
    "# チェーンに質問を投げて実行\n",
    "multi_query_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. 検索後の工夫\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、開発者がアプリケーションを簡単に構築できるようにすることを目的としており、オープンソースのコンポーネントやサードパーティの統合を利用して、LLMアプリケーションのライフサイクルの各段階を簡素化します。\\n\\n主な特徴には以下が含まれます：\\n\\n1. **標準化されたコンポーネントインターフェース**: 様々なAIアプリケーションのためのモデルや関連コンポーネントの多様なAPIを統一し、開発者がプロバイダー間で簡単に切り替えられるようにします。\\n\\n2. **オーケストレーション**: 複数のコンポーネントやモデルを効率的に接続し、複雑なアプリケーションを構築するための制御フローを提供します。\\n\\n3. **可観測性と評価**: アプリケーションの複雑さが増す中で、何が起こっているのかを理解しやすくし、開発者が迅速に評価を行えるようにします。\\n\\nLangChainは、個々のコンポーネントを単独で使用することも可能で、開発者は自分のユースケースに最適なコンポーネントを選択できます。また、LangGraphというオーケストレーションフレームワークを使用することで、状態を持つエージェントや複雑なアプリケーションを構築することができます。さらに、LangSmithを利用することで、アプリケーションのトレースや評価を行い、継続的な最適化とデプロイを支援します。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検索結果をスコアリングして成績の良い順に並び替えて、AI回答精度を上げるという仕組み\n",
    "\n",
    "# langchain_core.documents から Document クラスをインポート\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Reciprocal Rank Fusion（RRF）を実装する関数を定義\n",
    "def reciprocal_rank_fusion(retriever_outputs:list[list[Document]],k: int=60)->list[str]:\n",
    "\t# 各ドキュメントの内容とスコアを格納する辞書を初期化\n",
    "\tcontent_score_mapping={}\n",
    "\t# 各クエリの検索結果リストに対してループ\n",
    "\tfor docs in retriever_outputs:\n",
    "\t\t# 各ドキュメントとその順位に対してループ\n",
    "\t\tfor rank,doc in enumerate(docs):\n",
    "\t\t\t# ドキュメントの本文を取得\n",
    "\t\t\tcontent=doc.page_content\n",
    "\t\t\t# 初めて出現した内容ならスコアを0で初期化\n",
    "\t\t\tif content not in content_score_mapping:\n",
    "\t\t\t\tcontent_score_mapping[content]=0\n",
    "\t\t\t# RRFのスコア（1/(順位+k)）を加算\n",
    "\t\t\tcontent_score_mapping[content]+=1/(rank+k)\n",
    "\n",
    "\t# スコアの降順でソート\n",
    "\tranked=sorted(content_score_mapping.items(),key=lambda x:x[1], reverse=True)\n",
    "\t# ソート済みの内容のみをリストで返す\n",
    "\treturn [content for content, _ in ranked]\n",
    "\n",
    "rag_fusion_chain={\n",
    "\t\"question\":RunnablePassthrough(),\n",
    "\t\"context\":query_generation_chain|retriever.map()|reciprocal_rank_fusion\n",
    "}|prompt|model|StrOutputParser()\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere のリランクモデルを使用する準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere のリランクモデルの導入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。LangGraphを利用することで、状態を持つエージェントを作成し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなプロバイダーと統合し、標準化されたインターフェースを提供することで、開発者が異なるコンポーネントを簡単に切り替えたり、組み合わせたりできるようにします。また、複雑なアプリケーションのオーケストレーションをサポートするために、LangGraphというライブラリも提供しています。これにより、エージェントやマルチエージェントアプリケーションの構築が容易になります。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any# typingモジュールからAny型をインポート\n",
    "from langchain_cohere import CohereRerank# langchain_cohereパッケージからCohereRerankクラスをインポート\n",
    "from langchain_core.documents import Document# langchain_core.documentsからDocumentクラスをインポート\n",
    "\n",
    "# rerank関数を定義。入力は辞書型inpと、上位N件を指定するtop_n（デフォルト3）\n",
    "def rerank(inp:dict[str,Any], top_n:int=3)->list[Document]:\n",
    "\t# inpから質問文を取得\n",
    "\tquestion=inp[\"question\"]\n",
    "\t# inpからドキュメントリストを取得\n",
    "\tdocuments=inp[\"documents\"]\n",
    "\n",
    "\t# CohereRerankインスタンスを作成（モデル名とtop_nを指定） # model=\"rerank-multilingual-v3.0\", top_n=top_n\n",
    "\tcohere_reranker=CohereRerank(model=\"rerank-multilingual-v3.0\",top_n=top_n)\n",
    "\t# compress_documentsでリランキングを実行し、結果を返す # documents=documents, query=question\n",
    "\treturn cohere_reranker.compress_documents(documents=documents,query=question)\n",
    "\n",
    "# rerank_rag_chainを定義。RunnablePassthroughでquestionとdocumentsを渡し、rerankでcontextを追加し、プロンプト・モデル・出力パーサをパイプでつなぐ\n",
    "rerank_rag_chain=(\n",
    "\t{\n",
    "\t\t\"question\":RunnablePassthrough(),\n",
    "\t\t\"documents\":retriever\n",
    "\t}\n",
    "\t|RunnablePassthrough.assign(context=rerank)\n",
    "\t|prompt|model|StrOutputParser()\n",
    ")\n",
    "\n",
    "# rerank_rag_chainに「LangChainの概要を教えて」という質問を投げる\n",
    "rerank_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. 複数の Retriever を使う工夫\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM によるルーティング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/sf/2870kdw54dbg3z6dyhtwdhwr0000gq/T/ipykernel_9258/1066379075.py:25: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  route_prompt=ChatPromptTemplate.from_template(\"\"\"\\  # テンプレートからプロンプト作成\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever  # TavilySearchAPIRetrieverのインポート\n",
    "\n",
    "# retrieverに設定を付与してLangChainドキュメント用リトリーバーを作成\n",
    "langchain_document_retriever=retriever.with_config(  # retrieverにrun_nameを設定\n",
    "\t{\"run_name\":\"langchain_document_retriever\"}  # run_nameを指定\n",
    ")\n",
    "# TavilySearchAPIRetrieverでウェブ検索用リトリーバーを作成し設定を付与\n",
    "web_retriever=TavilySearchAPIRetriever(k=3).with_config(  # k=3で検索件数指定、run_nameを設定\n",
    "\t{\"run_name\":\"web_retriever\"}  # run_nameを指定\n",
    ")\n",
    "\n",
    "# Enumクラスをインポート（列挙型定義用）\n",
    "from enum import Enum  # Enumのインポート\n",
    "\n",
    "# どのリトリーバーを使うかを表す列挙型Routeを定義\n",
    "class Route(str, Enum):  # 文字列型のEnumを継承\n",
    "    langchain_document = \"langchain_document\"  # LangChainドキュメント用\n",
    "    web = \"web\"  # ウェブ検索用\n",
    "\n",
    "# モデル出力としてどのルートを選択したかを表すクラスRouteOutputを定義\n",
    "class RouteOutput(BaseModel):  # BaseModelを継承\n",
    "    route: Route  # routeフィールドにRoute型を指定\n",
    "\n",
    "# Retriever選択用のプロンプトを作成\n",
    "route_prompt=ChatPromptTemplate.from_template(\"\"\"\\  # テンプレートからプロンプト作成\n",
    "質問に回答するために適切なRetrieverを選択してください。\n",
    "質問: {question}\n",
    "\"\"\")  # {question}を埋め込む\n",
    "\n",
    "# プロンプト→モデル→出力整形のチェーンを作成\n",
    "route_chain=(  # チェーンの定義\n",
    "    route_prompt  # プロンプト\n",
    "    |model.with_structured_output(RouteOutput)  # モデル出力をRouteOutputで構造化\n",
    "    |(lambda x:x.route)  # routeフィールドのみ抽出\n",
    ")\n",
    "\n",
    "# ルートに応じて適切なリトリーバーを呼び出す関数を定義\n",
    "def routed_retriever(inp:dict[str,Any])->list[Document]:  # 入力は辞書型、出力はDocumentリスト\n",
    "\tquestion=inp[\"question\"]  # 質問文を取得\n",
    "\troute=inp[\"route\"]  # ルートを取得\n",
    "\n",
    "\tif route==Route.langchain_document:  # LangChainドキュメント用の場合\n",
    "\t\treturn langchain_document_retriever.invoke(question)  # LangChainドキュメントリトリーバーを呼び出し\n",
    "\telif route==Route.web:  # ウェブ検索用の場合\n",
    "\t\treturn web_retriever.invoke(question)  # ウェブリトリーバーを呼び出し\n",
    "\n",
    "\traise ValueError(f\"Unknown route: {route}\")  # 未知のルートの場合は例外\n",
    "\n",
    "# ルーティング付きRAGチェーンを定義\n",
    "route_rag_chain=(  # チェーンの定義\n",
    "\t{\n",
    "\t\t\"question\":RunnablePassthrough(),  # 質問をそのまま渡す\n",
    "\t\t\"route\":route_chain  # ルートはroute_chainで決定\n",
    "\t}\n",
    "\t|RunnablePassthrough.assign(context=routed_retriever)  # contextにrouted_retrieverの結果を追加\n",
    "\t|prompt|model|StrOutputParser()  # プロンプト→モデル→文字列出力パーサ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下の3つの主要なステージがあります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築します。LangGraphを利用することで、状態を持つエージェントを作成し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできるようにします。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換します。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクターストアなど）に対する標準インターフェースを実装し、数百のプロバイダーと統合しています。また、複数のオープンソースライブラリで構成されており、アーキテクチャやチュートリアル、APIリファレンスなどのリソースも提供されています。'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'東京の今日の天気は曇のち雨です。昼頃から所々で雨雲が湧き、夜には広く雨が降る予報です。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"東京の今日の天気は？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイブリッド検索の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能があります：\\n\\n- **開発**: LangChainのオープンソースコンポーネントやサードパーティ統合を使用してアプリケーションを構築できます。LangGraphを利用することで、状態を持つエージェントを作成し、ストリーミングや人間の介入をサポートします。\\n- **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n- **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクトルストアなど）に対する標準インターフェースを実装し、数百のプロバイダーと統合しています。また、複数のオープンソースライブラリで構成されており、さまざまな統合パッケージやコミュニティメンテナンスのリソースも提供しています。'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chroma_retriever=retriever.with_config(\n",
    "\t{\"run_name\":\"chroma_retriever\"}\n",
    ")\n",
    "bm25_retriever=BM25Retriever.from_documents(documents).with_config(\n",
    "\t{\"run_name\":\"bm25_retriever\"}\n",
    ")\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "hybrid_retriever=(\n",
    "\tRunnableParallel({\n",
    "\t\t\"chroma_documents\":chroma_retriever,\n",
    "\t\t\"bm25_documents\":bm25_retriever,\n",
    "\t})\n",
    "\t|(lambda x:[x[\"chroma_documents\"],x[\"bm25_documents\"]])\n",
    "\t|reciprocal_rank_fusion\n",
    ")\n",
    "hybrid_rag_chain=(\n",
    "\t{\n",
    "\t\t\"question\":RunnablePassthrough(),\n",
    "\t\t\"context\":hybrid_retriever\n",
    "\t}\n",
    "\t|prompt|model|StrOutputParser()\n",
    ")\n",
    "\n",
    "hybrid_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
